---
alwaysApply: false
description: Extension scaffold (Vite + TypeScript + React) and manifest/entry setup—apply when bootstrapping or restructuring entries.
---

# Decision Record: n8n Extension Scaffold

## Manifest V3 (baseline)
- name, version, description; `action` opens panel (optional); `options_page` for settings
- background: `service_worker` at `src/background/index.ts`
- content_scripts: inject on n8n editor pages; mount floating chatbot panel
- permissions: `storage`, `scripting`
- host_permissions (MVP): `http://localhost:5678/*`, `http://127.0.0.1:5678/*` (configurable later)
- web_accessible_resources: panel assets if needed

## Options Page (MVP)
- Provider selection: OpenAI (default), with abstraction for future providers
- Model selection: gpt-5 (default), with per-agent overrides supported later
- API key management: secure storage for provider credentials
- Feature toggles: basic settings for chatbot behavior

## UI & Build
- Framework: React + TypeScript (confirmed)
- Styling: Use n8n's CSS variables directly (no Tailwind)
- Build tool: Vite (browser + MV3 friendly)
- State: Zustand for lightweight state in panel
- Theme integration: Read n8n's computed CSS variables and apply to panel root

## Directory Structure (proposed)
- `src/background/` (service worker, orchestrator entry, messaging)
- `src/content/` (injection script, panel mount, page integration)
- `src/panel/` (React UI, chat, diffs, credential guides)
- `src/lib/` (llm provider iface, n8n client, tools, orchestrator graph)
- `src/options/` (settings page)
- `public/` (icons, static)

## n8n Target (MVP)
- Default host match: http://localhost:5678/* (add 127.0.0.1 variant)
- Detect canvas presence to show trigger button
- Auto-fetch workflows list on panel open; credentials fetched on demand

## Messaging
- content ↔ background: postMessage/Chrome runtime messages
- background executes LLM/tool calls as needed; panel streams responses

## CSP & Bundling
- Avoid `unsafe-eval`; load wasm-less stack; use fetch-based SDKs
- Model calls via background with streaming relay

## Open Items
- Exact host match patterns to include by default (localhost, 127.0.0.1, custom ports?)
- Whether to bundle Tailwind now or start with minimal CSS
- Options schema for provider/model keys (OpenAI gpt-5 default)
